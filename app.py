# -*- coding: utf-8 -*-
"""Bullying Detector Streamlit App (app.py)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ctVvh30m8aAV3-CDpuuAZTQjnjmADlGe
"""

import streamlit as st
import torch
from transformers import AutoTokenizer, BertForSequenceClassification
import pandas as pd
import plotly.express as px

# --- PAGE CONFIGURATION ---
st.set_page_config(
    page_title="SafeGuard AI - Bullying Detector",
    page_icon="üõ°Ô∏è",
    layout="centered",
    initial_sidebar_state="auto",
)

# --- CUSTOM CSS FOR A BETTER UI ---
st.markdown("""
<style>
    .stTextArea textarea {
        border: 2px solid #4A90E2;
        border-radius: 10px;
        font-size: 16px;
    }
    .stButton>button {
        background-color: #4A90E2;
        color: white;
        border-radius: 10px;
        border: none;
        padding: 10px 20px;
        font-size: 18px;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #357ABD;
    }
    .result-card {
        padding: 20px;
        border-radius: 10px;
        color: white;
        margin-top: 20px;
        text-align: center;
    }
    .bullying {
        background-color: #D9534F; /* Red for Bullying */
    }
    .non-bullying {
        background-color: #5CB85C; /* Green for Non-Bullying */
    }
    .result-text {
        font-size: 24px;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)


# --- MODEL AND TOKENIZER LOADING (with Caching) ---
# Using Streamlit's caching to load the model only once.
@st.cache_resource
def load_model():
    """
    Loads the fine-tuned BERT model and tokenizer from the local directory.
    """
    model_path = "./BullyingDeploymentPackage/final_model_v2"
    try:
        model = BertForSequenceClassification.from_pretrained(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        return model, tokenizer
    except Exception as e:
        st.error(f"Error loading model: {e}")
        st.error("Please make sure the 'BullyingDeploymentPackage/final_model' directory is in the same folder as app.py.")
        return None, None

model, tokenizer = load_model()

# --- PREDICTION FUNCTION ---
def predict(text):
    """
    Makes a prediction on the input text using the loaded model.
    """
    if model is None or tokenizer is None:
        return None, None

    # Put the model in evaluation mode
    model.eval()

    # Tokenize the input text
    inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors="pt")

    # Make prediction
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        # Apply softmax to get probabilities
        probabilities = torch.nn.functional.softmax(logits, dim=-1)
        predicted_class_id = torch.argmax(probabilities).item()
        confidence_scores = probabilities.flatten().tolist()

    return predicted_class_id, confidence_scores


# --- MAIN APP INTERFACE ---
st.title("üõ°Ô∏è SafeGuard AI")
st.subheader("A Real-Time Bullying and Toxicity Detector")
st.markdown("Enter any text below to check if it contains harmful content. This tool is built on a fine-tuned BERT model to help promote safer online interactions.")

# --- USER INPUT ---
user_text = st.text_area("Enter your text here:", height=150, placeholder="e.g., 'You are so smart!' or 'I hate this, it's terrible.'")

# --- PREDICTION BUTTON ---
if st.button("Analyze Text"):
    if user_text:
        with st.spinner("Analyzing..."):
            prediction_id, scores = predict(user_text)

            if prediction_id is not None:
                # Store prediction in session state to persist it
                st.session_state['prediction_id'] = prediction_id
                st.session_state['scores'] = scores

    else:
        st.warning("Please enter some text to analyze.")

# --- DISPLAY RESULTS ---
if 'prediction_id' in st.session_state:
    prediction_id = st.session_state['prediction_id']
    scores = st.session_state['scores']

    if prediction_id == 1: # Bullying
        st.markdown('<div class="result-card bullying"><p class="result-text">üö® Bullying Content Detected</p></div>', unsafe_allow_html=True)
    else: # Non-Bullying
        st.markdown('<div class="result-card non-bullying"><p class="result-text">‚úÖ Looks Safe</p></div>', unsafe_allow_html=True)

    # --- Display Confidence Scores ---
    st.write("### Prediction Confidence")
    data = {'Category': ['Non-Bullying', 'Bullying'], 'Confidence': scores}
    df_scores = pd.DataFrame(data)

    fig = px.bar(df_scores, x='Category', y='Confidence',
                 color='Category',
                 color_discrete_map={'Non-Bullying': '#5CB85C', 'Bullying': '#D9534F'},
                 text_auto='.2%',
                 template='plotly_white')
    fig.update_layout(
        showlegend=False,
        yaxis_title="Confidence Score",
        xaxis_title="",
    )
    st.plotly_chart(fig, use_container_width=True)


# --- "ABOUT THE MODEL" EXPANDER ---
with st.expander("üî¨ Learn More About The Model's Performance"):
    st.markdown("""
    This application is powered by a `bert-base-uncased` model that was fine-tuned on a diverse dataset of over 55,000 text samples labeled for toxicity and aggression.
    The model was trained to distinguish between two categories: 'Bullying' and 'Non-Bullying'. Here's a look at its performance on the unseen test data:
    """)
    # Display the confusion matrix image
    try:
        st.image("./BullyingDeploymentPackage/results/confusion_matrix.png", caption="Confusion Matrix on Test Data")
    except Exception as e:
        st.warning("Could not load confusion matrix image.")

    # Display the classification report
    try:
        with open("./BullyingDeploymentPackage/results/classification_report.txt") as f:
            report = f.read()
        st.text("Classification Report:")
        st.code(report, language='text')
    except Exception as e:
        st.warning("Could not load classification report.")

st.markdown("---")
st.markdown("Developed with ‚ù§Ô∏è by a student passionate about safe AI.")